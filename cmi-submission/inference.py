#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CMI â€“ Detect Behavior with Sensor Data
--------------------------------------
æ¨ç†ï¼ˆå¤šæäº¤åŒ… + å¤šåŸºæ¨¡å‹ + ä¸¥æ ¼ç±»åå¯¹é½ + æ— å›é€€ï¼‰

ä½ çš„ç›®å½•çº¦å®šï¼š
- å¤šä¸ªæäº¤åŒ…ï¼š/kaggle/input/cmi2025-ensemble/ensemble learning/cmi-submission (0.844), (0.846), (0.849)
- æ¯ä¸ªæäº¤åŒ…çš„ weights ç›®å½•ä¸­ï¼š
   model_fold1_imu.pth / model_fold2_imu.pth ... ï¼ˆæ—  full/imu å­æ–‡ä»¶å¤¹ï¼‰
   scaler_fold1_imu.pkl / spec_stats_fold1_imu.pkl / spec_params_fold1_imu.pkl ...
   ğŸ‘‰ æ–‡ä»¶åç»Ÿä¸€å½¢å¦‚ï¼š{tag}_fold{K}_{variant}.{ext}
- å…ƒæ¨¡å‹å·¥ä»¶ï¼ˆstackingï¼‰ï¼š/kaggle/input/cmi-link/stack_artifacts/{imu,full}/{meta_model.pkl, meta_info.json}
- åŸºæ¨¡å‹ç±»åæ–‡ä»¶ï¼ˆç”¨äºæ˜ å°„åˆ°å…ƒæ¨¡å‹ç±»ç©ºé—´ï¼‰ï¼Œä¼˜å…ˆæŸ¥æ‰¾ï¼ˆåœ¨æ¯ä¸ªæäº¤åŒ…çš„ weights ä¸‹ï¼‰ï¼š
   label_encoder_{variant}.pkl  æˆ–  classes_{variant}.json
   ï¼ˆè‹¥ä½ å¦æœ‰ label_map_{variant}.json ä¹Ÿä¼šè‡ªåŠ¨åº”ç”¨ï¼‰

æ³¨æ„ï¼šè„šæœ¬ä¸åšâ€œå›é€€ä¸ºå¹³å‡â€çš„é€»è¾‘ï¼Œç¼ºä»¶ç›´æ¥æŠ¥é”™ï¼Œä¾¿äºå°½å¿«å‘ç°é—®é¢˜ã€‚
"""

import os
import re
import json
import pickle
import warnings
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import polars as pl
import torch
from scipy import signal

warnings.filterwarnings("ignore")

# ------------------ ä½ çš„çœŸå®è·¯å¾„ï¼ˆæŒ‰ä½ è¦æ±‚ç¡¬ç¼–ç ï¼‰ ------------------
PACKS_PARENT_DIR = "/kaggle/input/cmi2025-ensemble/ensemble learning"
STACK_DIR        = "/kaggle/input/cmi-link/"

# ------------------ è‡ªå®šä¹‰æ¨¡å— ------------------
from models.multimodality import MultimodalityModel
from data_utils.data_preprocessing import (
    pad_sequences, feature_engineering, STATIC_FEATURE_COLS,
    generate_spectrogram, generate_feature_columns, add_tof_missing_flags
)
from data_utils.tof_utils import interpolate_tof

# ------------------ å¸¸é‡ ------------------
MAP_NON_TARGET = "Drink from bottle/cup"
VARIANTS       = ["full", "imu"]

# ------------------ å°å·¥å…· ------------------
def softmax_nd(z: np.ndarray) -> np.ndarray:
    z = z - np.max(z, axis=1, keepdims=True)
    ez = np.exp(z)
    return ez / np.sum(ez, axis=1, keepdims=True)

def _safe_clip01(x: np.ndarray, eps: float = 1e-15) -> np.ndarray:
    return np.clip(x, eps, 1 - eps)

def _ensure(p: str, what: str):
    if not os.path.exists(p):
        raise FileNotFoundError(f"ç¼ºå°‘ {what}: {p}")

def _list_submission_dirs(root: str) -> Dict[str, str]:
    """
    è¿”å› { '0.844': '<root>/cmi-submission (0.844)', ... }
    """
    dmap = {}
    if not os.path.isdir(root):
        raise FileNotFoundError(f"æ ¹è·¯å¾„ä¸å­˜åœ¨ï¼š{root}")
    for name in os.listdir(root):
        m = re.fullmatch(r"cmi-submission \((\d+\.\d{3})\)", name)
        if m:
            dmap[m.group(1)] = os.path.join(root, name)
    if not dmap:
        raise FileNotFoundError(f"åœ¨ {root} ä¸‹æœªæ‰¾åˆ°ä»»ä½• cmi-submission (X.XXX) ç›®å½•")
    return dmap

# ------------------ åŠ è½½å…ƒæ¨¡å‹ï¼ˆåªä» STACK_DIRï¼‰ ------------------
def _load_meta_for_variant(variant: str):
    mdir = os.path.join(STACK_DIR, "stack_artifacts", variant)
    model_path = os.path.join(mdir, "meta_model.pkl")
    info_path  = os.path.join(mdir, "meta_info.json")
    _ensure(model_path, f"[{variant}] å…ƒæ¨¡å‹")
    _ensure(info_path,  f"[{variant}] å…ƒä¿¡æ¯")
    import joblib
    est = joblib.load(model_path)
    with open(info_path, "r", encoding="utf-8") as f:
        meta_info = json.load(f)
    for k in ["feature_columns", "class_names_full"]:
        if k not in meta_info:
            raise ValueError(f"[{variant}] meta_info.json ç¼ºå°‘å­—æ®µ: {k} ({info_path})")
    print(f"âœ… [{variant}] å…ƒæ¨¡å‹æ¥è‡ª: {mdir}")
    return est, meta_info

def _list_bases_from_meta(meta_info: Dict) -> List[str]:
    """
    ä» meta_info['feature_columns'] å–åŸºæ¨¡å‹å‰ç¼€ï¼ˆä¿æŒé¡ºåºå»é‡ï¼‰
    å½¢å¦‚ ["0.849_imu.csv::ClassA", "0.846_imu.csv::ClassA", ...]
    """
    bases = []
    for col in meta_info["feature_columns"]:
        prefix = col.split("::", 1)[0]
        if prefix not in bases:
            bases.append(prefix)
    if not bases:
        raise ValueError("meta_info['feature_columns'] ä¸ºç©ºï¼Œæ— æ³•ç¡®å®šåŸºæ¨¡å‹åˆ—è¡¨")
    return bases

# ------------------ OOF åŸºæ¨¡å‹å‰ç¼€è§£æ -> æäº¤åŒ… + å˜ä½“ ------------------
def _parse_base_key(base_key: str) -> Tuple[str, str]:
    """
    base_key: "0.849_imu.csv" -> ('0.849', 'imu')
              "0.846_full.csv" -> ('0.846', 'full')
    """
    m = re.fullmatch(r"(\d+\.\d{3})_(imu|full)\.csv", base_key)
    if not m:
        raise ValueError(f"æ— æ³•è§£æåŸºæ¨¡å‹å‰ç¼€: {base_key}ï¼ˆæœŸæœ›æ ¼å¼ '0.849_imu.csv'ï¼‰")
    return m.group(1), m.group(2)

def _weights_dir_for_key(base_key: str) -> Tuple[str, str]:
    """
    ç”± base_key ç¡®å®šå…¶æƒé‡ç›®å½•ï¼ˆæäº¤åŒ…çš„ weights æ ¹ç›®å½•ï¼‰å’Œæäº¤åŒ…æ ¹è·¯å¾„ï¼š
      -> <PACKS_PARENT_DIR>/cmi-submission (lb)/weights/
    """
    lb, variant = _parse_base_key(base_key)
    dmap = _list_submission_dirs(PACKS_PARENT_DIR)
    if lb not in dmap:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æäº¤åŒ…ç›®å½•ï¼šcmi-submission ({lb})")
    pack_root = dmap[lb]
    wdir = os.path.join(pack_root, "weights")
    if not os.path.isdir(wdir):
        raise FileNotFoundError(f"æƒé‡ç›®å½•ä¸å­˜åœ¨ï¼š{wdir}")
    return wdir, pack_root

# ------------------ åŸºæ¨¡å‹ç±»ä¿¡æ¯ï¼ˆåœ¨ weights æ ¹ç›®å½•ï¼ŒæŒ‰ variant åŒºåˆ†ï¼‰ ------------------
def _load_base_classes(weights_dir: str, variant: str) -> List[str]:
    """
    ä¼˜å…ˆï¼šweights/label_encoder_{variant}.pkl
    æ¬¡é€‰ï¼šweights/classes_{variant}.json
    å†æ¬¡ï¼šweights/label_encoder.pkl æˆ– weights/classes.json
    """
    cand = [
        os.path.join(weights_dir, f"label_encoder_{variant}.pkl"),
        os.path.join(weights_dir, f"classes_{variant}.json"),
        os.path.join(weights_dir, "label_encoder.pkl"),
        os.path.join(weights_dir, "classes.json"),
    ]
    for p in cand:
        if not os.path.exists(p):
            continue
        if p.endswith(".pkl"):
            with open(p, "rb") as f:
                le = pickle.load(f)
            cls = list(le.classes_)
            if not cls:
                raise ValueError(f"[{p}] classes_ ä¸ºç©º")
            return [str(x) for x in cls]
        else:  # .json
            with open(p, "r", encoding="utf-8") as f:
                arr = json.load(f)
            if not isinstance(arr, list) or not all(isinstance(x, str) for x in arr) or not arr:
                raise ValueError(f"[{p}] å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²æ•°ç»„")
            return arr
    raise FileNotFoundError(
        f"[{weights_dir}] æœªæ‰¾åˆ°ç±»åæ–‡ä»¶ï¼ˆéœ€è¦ label_encoder_{variant}.pkl æˆ– classes_{variant}.jsonï¼›"
        f"ä¹Ÿå¯æä¾› label_encoder.pkl / classes.jsonï¼‰"
    )

def _load_label_map(weights_dir: str, variant: str) -> Dict[str, str]:
    """
    å¯é€‰ï¼šweights/label_map_{variant}.json æˆ– weights/label_map.json
    """
    for name in [f"label_map_{variant}.json", "label_map.json"]:
        p = os.path.join(weights_dir, name)
        if os.path.exists(p):
            with open(p, "r", encoding="utf-8") as f:
                d = json.load(f)
            if not isinstance(d, dict):
                raise ValueError(f"[{p}] éœ€ä¸ºå­—å…¸ {{åŸºç±»: å…ƒç±»}}")
            return {str(k): str(v) for k, v in d.items()}
    return {}

# ------------------ æ¨¡å‹åŠ è½½ï¼ˆæŒ‰æ–‡ä»¶ååç¼€ _{variant} + fold å·ï¼‰ ------------------
def _load_models_from_weights_dir(device, weights_dir: str, variant: str) -> List[Tuple]:
    """
    éœ€è¦æ–‡ä»¶ï¼š
      model_fold{K}_{variant}.pth
      scaler_fold{K}_{variant}.pkl
      spec_stats_fold{K}_{variant}.pkl
      spec_params_fold{K}_{variant}.pkl
    ä¹Ÿå…¼å®¹ä¸­é—´å¸¦ä¸‹åˆ’çº¿ï¼šfold_{K}
    """
    files = os.listdir(weights_dir)

    # è¯†åˆ«å¯ç”¨çš„ fold å·
    fold_nums = set()
    pat_model = re.compile(rf"^model_fold_?(\d+)_({variant})\.pth$")
    pat_model2 = re.compile(rf"^model_fold(\d+)_({variant})\.pth$")
    for f in files:
        m = pat_model.match(f) or pat_model2.match(f)
        if m:
            fold_nums.add(int(m.group(1)))
    fold_nums = sorted(list(fold_nums))
    if not fold_nums:
        raise FileNotFoundError(f"[{weights_dir}] æœªå‘ç°ä»»ä½• model_foldK_{variant}.pth")

    def _pick(tag: str, k: int, must_ext: str):
        # å…è®¸ fold_{k} æˆ– fold{k}
        cand = [f for f in files if f.endswith(must_ext) and re.fullmatch(rf"{tag}_fold_?{k}_{variant}\{must_ext}", f)]
        if not cand:
            # ä¸€äº›äººå‘½åæ˜¯ f"{tag}_fold{k}_{variant}.pkl"
            cand = [f for f in files if f.endswith(must_ext) and re.fullmatch(rf"{tag}_fold{k}_{variant}\{must_ext}", f)]
        if not cand:
            raise FileNotFoundError(f"[{weights_dir}] ç¼ºå°‘ {tag}_fold{k}_{variant}{must_ext}")
        return os.path.join(weights_dir, cand[0])

    folds = []
    for k in fold_nums:
        # è·¯å¾„
        model_path = None
        for f in files:
            if pat_model.match(f) or pat_model2.match(f):
                m = (pat_model.match(f) or pat_model2.match(f))
                if int(m.group(1)) == k:
                    model_path = os.path.join(weights_dir, f)
                    break
        if model_path is None:
            raise FileNotFoundError(f"[{weights_dir}] æœªæ‰¾åˆ° model_fold{k}_{variant}.pth")

        scaler_path      = _pick("scaler",      k, ".pkl")
        spec_stats_path  = _pick("spec_stats",  k, ".pkl")
        spec_params_path = _pick("spec_params", k, ".pkl")

        with open(scaler_path, "rb") as f:
            scaler = pickle.load(f)
        with open(spec_stats_path, "rb") as f:
            spec_stats = pickle.load(f)
        with open(spec_params_path, "rb") as f:
            spec_params = pickle.load(f)

        ckpt = torch.load(model_path, map_location=device)
        if 'model_cfg' not in ckpt:
            raise ValueError(f"[{model_path}] ç¼ºå°‘ 'model_cfg'")
        model_cfg = {k2: v2 for k2, v2 in ckpt['model_cfg'].items() if k2 != 'type'}
        state_dict = ckpt['state_dict']
        model = MultimodalityModel(**model_cfg)

        # å¤„ç† torch.compile ä¿å­˜çš„ _orig_mod å‰ç¼€
        is_compiled = any(key.startswith('_orig_mod.') for key in state_dict.keys())
        if is_compiled:
            from collections import OrderedDict
            state_dict = OrderedDict((k.replace('_orig_mod.', '', 1), v) for k, v in state_dict.items())
        model.load_state_dict(state_dict, strict=True)
        model.to(device).eval()
        try:
            model = torch.compile(model, mode="reduce-overhead")
        except Exception as e:
            print(f"âš ï¸  torch.compile skipped: {e}")

        # æ¨æ–­ seq_len
        seq_len = None
        if 'sequence_length' in model_cfg: seq_len = model_cfg['sequence_length']
        elif 'seq_len' in model_cfg:      seq_len = model_cfg['seq_len']
        elif 'tof_branch_cfg' in model_cfg and 'seq_len' in model_cfg['tof_branch_cfg']:
            seq_len = model_cfg['tof_branch_cfg']['seq_len']

        # æ ¡éªŒ spec_params çš„ max_length
        try:
            max_len_from_spec = spec_params.get('max_length', None)
        except Exception:
            max_len_from_spec = None
        if (seq_len is not None) and (max_len_from_spec is not None) and (max_len_from_spec != seq_len):
            raise ValueError(f"[{weights_dir}] fold {k} é•¿åº¦ä¸ä¸€è‡´: model seq_len={seq_len}, spec.max_length={max_len_from_spec}")

        folds.append((model, scaler, spec_stats, spec_params, seq_len))
    return folds

# ------------------ å˜ä½“åˆ¤å®š & é¢„å¤„ç† ------------------
def _decide_variant(seq_df: "pd.DataFrame") -> str:
    thm_cols = [c for c in seq_df.columns if c.startswith("thm_")]
    tof_cols = [c for c in seq_df.columns if c.startswith("tof_")]
    if not thm_cols and not tof_cols:
        return "imu"
    thm_all_missing = not seq_df[thm_cols].notna().values.any() if thm_cols else True
    tof_all_missing = not seq_df[tof_cols].notna().values.any() if tof_cols else True
    return "imu" if thm_all_missing and tof_all_missing else "full"

def preprocess_single_sequence(seq_pl: pl.DataFrame, demog_pl: pl.DataFrame):
    seq_df = seq_pl.to_pandas()
    if not demog_pl.is_empty():
        seq_df = seq_df.merge(demog_pl.to_pandas(), on="subject", how="left")

    variant = _decide_variant(seq_df)
    if variant != "imu":
        seq_df = add_tof_missing_flags(seq_df)
        seq_df = interpolate_tof(seq_df)

    processed_df, feature_cols = feature_engineering(seq_df)

    existing_static_cols = [c for c in STATIC_FEATURE_COLS if c in processed_df.columns]
    for col in existing_static_cols:
        if col not in feature_cols:
            feature_cols.append(col)

    dynamic_missing_flags = [c for c in processed_df.columns if c.endswith('_missing')]
    for col in dynamic_missing_flags:
        if col not in feature_cols:
            feature_cols.append(col)

    if variant == "imu":
        feature_cols = [c for c in feature_cols if not (c.startswith("thm_") or c.startswith("tof_"))]

    final_features_df = processed_df.sort_values("sequence_counter")
    final_features_df = final_features_df[[c for c in feature_cols if c in final_features_df.columns]]
    return variant, final_features_df

# ------------------ å•æŠ˜æ¨ç†ï¼ˆä¸è®­ç»ƒæŠ˜ä¸€è‡´çš„ scaler/specï¼‰ ------------------
def _predict_one_fold(
    fold_entry: Tuple, features_df: pd.DataFrame, device: torch.device
) -> np.ndarray:
    model, scaler, spec_stats, spec_params, sequence_length = fold_entry

    # 1) æ ‡å‡†åŒ–
    X_scaled_unpadded = scaler.transform(features_df).astype(np.float32)
    scaled_feature_names = scaler.get_feature_names_out().tolist()

    # 2) åˆ—åˆ†æ´¾
    static_cols = [c for c in scaled_feature_names if c in STATIC_FEATURE_COLS or c.endswith('_missing')]
    thm_cols, tof_cols = generate_feature_columns(scaled_feature_names)
    thm_cols = [c for c in thm_cols if c in scaled_feature_names]
    tof_cols = [c for c in tof_cols if c in scaled_feature_names]
    spec_source_cols = ['linear_acc_x','linear_acc_y','linear_acc_z','angular_vel_x','angular_vel_y','angular_vel_z']
    spec_source_cols = [c for c in spec_source_cols if c in scaled_feature_names]
    imu_cols = [c for c in scaled_feature_names if c not in static_cols + tof_cols + thm_cols]

    static_idx = [scaled_feature_names.index(c) for c in static_cols]
    tof_idx    = [scaled_feature_names.index(c) for c in tof_cols]
    thm_idx    = [scaled_feature_names.index(c) for c in thm_cols]
    imu_idx    = [scaled_feature_names.index(c) for c in imu_cols]
    spec_idx   = [scaled_feature_names.index(c) for c in spec_source_cols]

    static_arr = X_scaled_unpadded[0:1, static_idx]
    tof_arr, thm_arr, imu_arr = X_scaled_unpadded[:, tof_idx], X_scaled_unpadded[:, thm_idx], X_scaled_unpadded[:, imu_idx]

    # TOF mask
    tof_sensor_ids = []
    for c in scaled_feature_names:
        if c.startswith('tof_') and c.endswith('_missing') and c.count('_') == 2:
            try:
                sid = int(c.split('_')[1])
                if sid not in tof_sensor_ids: tof_sensor_ids.append(sid)
            except Exception: pass
    if not tof_sensor_ids:
        seen = set()
        for c in scaled_feature_names:
            if c.startswith('tof_') and '_v' in c and not c.endswith('_missing'):
                try: seen.add(int(c.split('_')[1]))
                except Exception: pass
        tof_sensor_ids = sorted(list(seen))
    ch_mask_vals = []
    for sid in sorted(tof_sensor_ids):
        flag = f"tof_{sid}_missing"
        if flag in scaled_feature_names:
            flag_idx = scaled_feature_names.index(flag)
            valid = 1.0 - float(X_scaled_unpadded[0, flag_idx])
        else:
            valid = 1.0
        ch_mask_vals.append(valid)
    tof_channel_mask_arr = np.array(ch_mask_vals, dtype=np.float32)[np.newaxis, :]
    spec_source_arr = X_scaled_unpadded[:, spec_idx]

    # 3) Padding
    X_imu_pad, imu_mask = pad_sequences([imu_arr], max_length=sequence_length)
    X_thm_pad, _        = pad_sequences([thm_arr], max_length=sequence_length)
    X_tof_pad, _        = pad_sequences([tof_arr], max_length=sequence_length)

    # 4) é¢‘è°±å›¾
    sequence_spectrograms = []
    spec_mean, spec_std = spec_stats['mean'], spec_stats['std']
    for i in range(spec_source_arr.shape[1]):
        signal_1d = spec_source_arr[:, i]
        seq_len_current = len(signal_1d)
        if seq_len_current >= sequence_length:
            padded_signal = signal_1d[-sequence_length:]
        else:
            padded_signal = np.pad(signal_1d, (sequence_length - seq_len_current, 0), 'constant')
        spec = generate_spectrogram(
            padded_signal,
            fs=spec_params['fs'],
            nperseg=spec_params['nperseg'],
            noverlap=spec_params['noverlap'],
            max_length=sequence_length,
        )
        spec_norm = ((spec - spec_mean) / (spec_std + 1e-6)).astype(np.float32)
        sequence_spectrograms.append(spec_norm)
    X_spec = np.stack(sequence_spectrograms, axis=0).astype(np.float32)[np.newaxis, ...]

    # 5) Tensor
    xb_imu = torch.from_numpy(X_imu_pad).to(DEVICE)
    xb_thm = torch.from_numpy(X_thm_pad).to(DEVICE)
    xb_tof = torch.from_numpy(X_tof_pad).to(DEVICE)
    xb_spec = torch.from_numpy(X_spec).to(DEVICE)
    xb_static = torch.from_numpy(static_arr).to(DEVICE)
    xb_mask = torch.from_numpy(imu_mask).to(DEVICE)

    # THM mask
    thm_sensor_ids_inf = []
    for c in scaled_feature_names:
        if c.startswith('thm_') and c.endswith('_missing'):
            try: thm_sensor_ids_inf.append(int(c.split('_')[1]))
            except Exception: pass
    thm_sensor_ids_inf = sorted(list(set(thm_sensor_ids_inf)))
    thm_mask_vals = []
    for sid in thm_sensor_ids_inf:
        flag = f"thm_{sid}_missing"
        if flag in scaled_feature_names:
            idx_flag = scaled_feature_names.index(flag)
            thm_mask_vals.append(1.0 - float(X_scaled_unpadded[0, idx_flag]))
        else:
            thm_mask_vals.append(1.0)
    thm_channel_mask_arr = np.array(thm_mask_vals, dtype=np.float32)[np.newaxis, :] if len(thm_mask_vals) > 0 else np.ones((1,0), dtype=np.float32)
    xb_thm_ch_mask = torch.from_numpy(thm_channel_mask_arr).to(DEVICE)

    # IMU rot-only mask
    imu_feature_names_inf = [c for c in scaled_feature_names if c not in static_cols + tof_cols + thm_cols]
    rot_fields_inf = ['rot_w','rot_x','rot_y','rot_z']
    imu_rot_mask = np.ones((1, len(imu_feature_names_inf)), dtype=np.float32)
    if 'rot_missing' in scaled_feature_names:
        idx_rm = scaled_feature_names.index('rot_missing')
        if float(X_scaled_unpadded[0, idx_rm]) == 1.0:
            for i, name in enumerate(imu_feature_names_inf):
                if name in rot_fields_inf:
                    imu_rot_mask[0, i] = 0.0
    xb_imu_ch_mask = torch.from_numpy(imu_rot_mask).to(DEVICE)

    # 6) å‰å‘
    with torch.no_grad():
        logits = model(
            xb_imu, xb_thm, xb_tof, xb_spec, xb_static,
            mask=xb_mask,
            tof_channel_mask=torch.from_numpy(tof_channel_mask_arr).to(DEVICE),
            thm_channel_mask=xb_thm_ch_mask,
            imu_channel_mask=xb_imu_ch_mask
        )
        probs = torch.softmax(logits, dim=1).cpu().numpy()
    return probs  # [1, C_base]

def _avg_proba_for_base(fold_entries: List[Tuple], features_df: pd.DataFrame) -> np.ndarray:
    acc = None
    for fe in fold_entries:
        p = _predict_one_fold(fe, features_df, DEVICE)  # [1, C_base]
        acc = p if acc is None else acc + p
    avg = acc / len(fold_entries)
    return avg[0]  # [C_base]

def _map_base_to_meta_proba(
    proba_base: np.ndarray,
    base_classes: List[str],
    meta_classes: List[str],
    base_to_meta_map: Dict[str, str]
) -> np.ndarray:
    """
    å°†â€œåŸºæ¨¡å‹ç±»ç©ºé—´â€çš„æ¦‚ç‡æ˜ å°„åˆ°â€œå…ƒæ¨¡å‹ç±»ç©ºé—´â€ã€‚
    """
    meta_idx = {name: i for i, name in enumerate(meta_classes)}
    out = np.zeros((len(meta_classes),), dtype=np.float32)

    for j, bname in enumerate(base_classes):
        mapped = base_to_meta_map.get(bname, bname)
        if mapped not in meta_idx:
            raise ValueError(
                f"ç±»åæ˜ å°„å¤±è´¥ï¼šåŸºç±» '{bname}'ï¼ˆæ˜ å°„ä¸º '{mapped}'ï¼‰ä¸åœ¨å…ƒæ¨¡å‹ç±»ç©ºé—´ä¸­ã€‚"
            )
        out[meta_idx[mapped]] = float(proba_base[j])

    s = out.sum()
    if s > 0:
        out = out / s
    return out

# ------------------ åˆå§‹åŒ–èµ„æºï¼ˆä¸¥æ ¼ï¼Œæ— å›é€€ï¼‰ ------------------
print("ğŸ”§  Initialising inference resources â€¦")
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

RESOURCES: Dict[str, Dict] = {}
META: Dict[str, Dict] = {}  # {variant: {"est":..., "info":...}}

# 1) åŠ è½½å…ƒæ¨¡å‹ï¼ˆä»…ä» STACK_DIRï¼‰
for v in VARIANTS:
    est, meta_info = _load_meta_for_variant(v)
    META[v] = {"est": est, "info": meta_info}
    print(f"âœ… [{v}] meta loaded with {len(_list_bases_from_meta(meta_info))} base(s).")

# 2) æ ¹æ® meta å†³å®šè¦åŠ è½½çš„æ‰€æœ‰åŸºæ¨¡å‹ï¼ˆè·¨å¤šä¸ªæäº¤åŒ…ï¼‰
packs = _list_submission_dirs(PACKS_PARENT_DIR)

for v in VARIANTS:
    meta_info = META[v]["info"]
    base_keys = _list_bases_from_meta(meta_info)
    class_names_full = meta_info["class_names_full"]

    bases: Dict[str, Dict] = {}
    for bk in base_keys:
        lb, variant_b = _parse_base_key(bk)
        # æˆ‘ä»¬åªåŠ è½½ä¸å½“å‰ variant ç›¸åŒçš„ baseï¼ˆå¦ä¸€ variant çš„ base åœ¨è¯¥ variant é¢„æµ‹æ—¶ä¼šè¢«å¿½ç•¥ï¼‰
        if variant_b != v:
            continue

        weights_dir, pack_root = _weights_dir_for_key(bk)
        base_classes = _load_base_classes(weights_dir, v)
        label_map    = _load_label_map(weights_dir, v)
        fold_entries = _load_models_from_weights_dir(DEVICE, weights_dir, v)

        bases[bk] = {
            "fold_entries": fold_entries,
            "base_classes": base_classes,
            "label_map": label_map,
            "weights_dir": weights_dir,
        }
        print(f"  â€¢ [{v}] {bk} -> {weights_dir} (folds={len(fold_entries)})")

    if not bases:
        raise RuntimeError(f"[{v}] æœªæ‰¾åˆ°ä»»ä½•åŸºæ¨¡å‹ï¼Œè¯·æ£€æŸ¥ meta_info['feature_columns'] ä¸ç›®å½•ç»“æ„ã€‚")

    RESOURCES[v] = {
        "bases": bases,
        "meta": META[v],
    }

print("âœ…  Resource initialization complete. Ready for inference.")

# ------------------ é¢„æµ‹ï¼ˆä¸¥æ ¼ stackingï¼‰ ------------------
def _predict_with_stacking(variant: str, features_df: pd.DataFrame) -> Tuple[np.ndarray, List[str]]:
    res_v = RESOURCES[variant]
    est = res_v["meta"]["est"]
    meta_info = res_v["meta"]["info"]
    meta_classes = meta_info["class_names_full"]

    # é€ baseï¼ˆæŒ‰ meta çš„é¡ºåºï¼‰å–æ¦‚ç‡å¹¶æ˜ å°„åˆ°å…ƒç©ºé—´
    probs_by_base: Dict[str, np.ndarray] = {}
    for col in meta_info["feature_columns"]:
        bk = col.split("::", 1)[0]
        if bk in probs_by_base:
            continue
        if bk not in res_v["bases"]:
            # è¿™ä¸ªå‰ç¼€å¯èƒ½å±äºå¦ä¸€ variantï¼Œå½“å‰ variant è·³è¿‡
            continue
        base_entry = res_v["bases"][bk]
        p_base = _avg_proba_for_base(base_entry["fold_entries"], features_df)  # [C_base]
        p_meta = _map_base_to_meta_proba(p_base, base_entry["base_classes"], meta_classes, base_entry["label_map"])
        probs_by_base[bk] = p_meta

    # æŒ‰ feature_columns æ‹¼ stacking ç‰¹å¾
    x_stack = np.zeros((1, len(meta_info["feature_columns"])), dtype=np.float32)
    cls_to_idx = {c: i for i, c in enumerate(meta_classes)}
    for j, col in enumerate(meta_info["feature_columns"]):
        prefix, cls = col.split("::", 1)
        if prefix not in probs_by_base:
            # å±äºå¦ä¸€ variant çš„åˆ—ç½® 0
            x_stack[0, j] = 0.0
        else:
            x_stack[0, j] = float(probs_by_base[prefix][cls_to_idx[cls]])

    # å…ƒæ¨¡å‹è¾“å‡ºæ¦‚ç‡
    if hasattr(est, "predict_proba"):
        proba = est.predict_proba(x_stack)
        if isinstance(proba, list):
            proba = np.column_stack([p[:, 1] if p.ndim == 2 else p for p in proba])
    else:
        if hasattr(est, "decision_function"):
            df = est.decision_function(x_stack)
            if df.ndim == 1:
                df = np.stack([-df, df], axis=1)
            proba = softmax_nd(df)
        else:
            raise RuntimeError("å…ƒæ¨¡å‹æ—¢æ—  predict_proba ä¹Ÿæ—  decision_functionã€‚")

    proba = _safe_clip01(np.asarray(proba))
    return proba, meta_classes

def predict(sequence: pl.DataFrame, demographics: pl.DataFrame) -> str:
    if sequence.is_empty():
        return MAP_NON_TARGET

    variant, features_df = preprocess_single_sequence(sequence, demographics)
    proba, class_names = _predict_with_stacking(variant, features_df)
    pred_idx = int(np.argmax(proba, axis=1)[0])
    pred_name = class_names[pred_idx]
    # è¯„æµ‹ä¸æ¥å— "Other"ï¼›è‹¥å‘½ä¸­å°±æ”¹æˆæ¯”èµ›è¦æ±‚çš„åå­—
    if pred_name.strip().lower() == "other":
        pred_name = "Drink from bottle/cup"
    return pred_name


# ------------------ å¯åŠ¨è¯„æµ‹æœåŠ¡å™¨ ------------------
if __name__ == "__main__":
    import kaggle_evaluation.cmi_inference_server as kis
    print("ğŸš€ Starting CMIInferenceServer â€¦")
    inference_server = kis.CMIInferenceServer(predict)
    if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):
        inference_server.serve()
    else:
        # æœ¬åœ°æµ‹è¯•ç½‘å…³
        os.chdir("/kaggle/working")
        inference_server.run_local_gateway(
            data_paths=(
                "/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv",
                "/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv"
            )
        )
